{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCnh7txruDwL"
   },
   "source": [
    "# Поиск аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6II8JMnuDwO"
   },
   "source": [
    "Методы обнаружения аномалий, как следует из названия, позволяют находить необычные объекты в выборке. Но что такое \"необычные\" и совпадает ли это определение у разных методов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFjf40SeuDwO"
   },
   "source": [
    "Начнём с поиска аномалий в текстах: научимся отличать вопросы о программировании от текстов из 20newsgroups про религию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIwHXueZuDwO"
   },
   "source": [
    "Подготовьте данные: в обучающую выборку возьмите 20 тысяч текстов из датасета Stack Overflow, а тестовую выборку сформируйте из 10 тысяч текстов со Stack Overflow и 100 текстов из класса soc.religion.christian датасета 20newsgroups (очень пригодится функция `fetch_20newsgroups(categories=['soc.religion.christian'])`). Тексты про программирование будем считать обычными, а тексты про религию — аномальными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJgW4zVZuDwP",
    "outputId": "db651d1c-0078-42c0-e6b6-5003582548ec"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "categories_normal = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
    "                     'comp.sys.mac.hardware', 'comp.windows.x']\n",
    "data_stack = fetch_20newsgroups(subset='all', categories=categories_normal,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "limit_train = 20000\n",
    "if len(data_stack.data) > limit_train:\n",
    "    indices = np.random.choice(len(data_stack.data), limit_train, replace=False)\n",
    "    train_texts = [data_stack.data[i] for i in indices]\n",
    "else:\n",
    "    train_texts = data_stack.data\n",
    "data_religion = fetch_20newsgroups(subset='all', categories=['soc.religion.christian'],\n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "if len(data_stack.data) > limit_train + 10000:\n",
    "    mask = np.ones(len(data_stack.data), dtype=bool)\n",
    "    if 'indices' in locals(): mask[indices] = False\n",
    "    available_indices = np.where(mask)[0]\n",
    "    test_normal_indices = np.random.choice(available_indices, 10000, replace=False)\n",
    "    test_normal_texts = [data_stack.data[i] for i in test_normal_indices]\n",
    "else:\n",
    "    test_normal_texts = data_stack.data[:10000]\n",
    "\n",
    "test_anomaly_texts = data_religion.data[:100]\n",
    "\n",
    "y_test = np.array([0]*len(test_normal_texts) + [1]*len(test_anomaly_texts))\n",
    "test_texts = test_normal_texts + test_anomaly_texts\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}\")\n",
    "print(f\"Test size (Normal): {len(test_normal_texts)}, (Anomaly): {len(test_anomaly_texts)}\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0cVmrBDuDwP"
   },
   "source": [
    "**(1 балл)**\n",
    "\n",
    "Проверьте качество выделения аномалий (precision и recall на тестовой выборке, если считать аномалии положительным классов, а обычные тексты — отрицательным) для IsolationForest. В качестве признаков используйте TF-IDF, где словарь и IDF строятся по обучающей выборке. Не забудьте подобрать гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C0ZzIGFuDwQ",
    "outputId": "7909d5f2-c1c0-4c87-9880-1b0ab4fa29b0"
   },
   "outputs": [],
   "source": [
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred_raw = clf.predict(X_test)\n",
    "\n",
    "y_pred = np.where(y_pred_raw == -1, 1, 0)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EEXad6euDwQ"
   },
   "source": [
    "**(5 баллов)**\n",
    "\n",
    "Скорее всего, качество оказалось не на высоте. Разберитесь, в чём дело:\n",
    "* посмотрите на тексты, которые выделяются как аномальные, а также на слова, соответствующие их ненулевым признакам\n",
    "* изучите признаки аномальных текстов\n",
    "* посмотрите на тексты из обучающей выборки, ближайшие к аномальным; действительно ли они похожи по признакам?\n",
    "\n",
    "Сделайте выводы и придумайте, как избавиться от этих проблем. Предложите варианты двух типов: (1) в рамках этих же признаков (но которые, возможно, будут считаться по другим наборам данных) и методов и (2) без ограничений на изменения. Реализуйте эти варианты и проверьте их качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYvrEzFGuDwR",
    "outputId": "5c4fc69e-7576-41ee-f3d1-24044c530f5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "anomaly_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "for i in anomaly_indices[:5]:\n",
    "    label = \"Anomaly\" if y_test[i] == 1 else \"Normal (False Positive)\"\n",
    "    print(f\"--- {label} ---\")\n",
    "    print(test_texts[i][:200])\n",
    "    print(\"\\n\")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "first_anomaly_vec = X_test[anomaly_indices[0]]\n",
    "non_zero_indices = first_anomaly_vec.nonzero()[1]\n",
    "top_words = [feature_names[i] for i in non_zero_indices]\n",
    "print(f\"Слова в первом найденном аномальном объекте: {top_words[:10]}\")\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute', metric='cosine').fit(X_train)\n",
    "distances, indices = nbrs.kneighbors(first_anomaly_vec)\n",
    "\n",
    "print(\"\\nБлижайшие соседи из обучающей выборки (StackOverflow):\")\n",
    "for i in range(3):\n",
    "    print(f\"Neighbor {i+1}: {train_texts[indices[0][i]][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHk8G7mLuDwR"
   },
   "source": [
    "### Эксперимент только с изменением датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bj0a2TOCuDwR",
    "outputId": "7ba08a53-e6aa-49bd-e7f7-c391ab0cdba6"
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer_v2 = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.8,\n",
    "    min_df=5,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_v2 = vectorizer_v2.fit_transform(train_texts)\n",
    "X_test_v2 = vectorizer_v2.transform(test_texts)\n",
    "\n",
    "clf_v2 = IsolationForest(n_estimators=100, contamination=0.05, random_state=42, n_jobs=-1)\n",
    "clf_v2.fit(X_train_v2)\n",
    "\n",
    "scores = clf_v2.decision_function(X_test_v2)\n",
    "\n",
    "threshold = np.percentile(scores, 1)\n",
    "y_pred_v2 = np.where(scores < threshold, 1, 0)\n",
    "\n",
    "precision_v2 = precision_score(y_test, y_pred_v2)\n",
    "recall_v2 = recall_score(y_test, y_pred_v2)\n",
    "\n",
    "print(f\"V2 Precision: {precision_v2:.4f}\")\n",
    "print(f\"V2 Recall: {recall_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_53_1ftuDwR"
   },
   "source": [
    "### Эксперимент с любыми изменениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 557,
     "referenced_widgets": [
      "4443b5cd479949c6bbc1b9e922407910",
      "a604f74fe3ca4c389b62539633cadaaa",
      "3724734183114937b73606c43035d1c5",
      "76b482d6436d4b1a96af413d134404d3",
      "367b9980650b43c3bd421ff5d7e5929c",
      "b5eb6ceed93d4356be20a16af771a02c",
      "0b975c208b1e44738ac4c0c749e7dfc8",
      "e0f1077c03b741c99da500c671c4de5a",
      "d9162b3304d645f8b30613c0bf61c41d",
      "45e4672c59a64268b11f9fe3bb5f140b",
      "10af019e9bc1414c90d5f24b251aa9ba",
      "cc6e3c4d0b904cb0881b585a62aed7e6",
      "0d5654aac4a34126bb817d1e1679bf69",
      "e4097fbbeea44b1f998ca5bf0f9af936",
      "570b4b366b324adda2d2702442057cff",
      "a9d9323638904c939e280ec1224f3af5",
      "9d6383318f844edbba7d3b3ed6cbfc37",
      "a0fc2b10617a4f0ba7d38f20112006cb",
      "7bf42e547d4c42f4a84788893a33998e",
      "1e4c38a1e1d9417f9e898770f5f3f475",
      "b9406d97288b45ab9c4b377b0ccbf16b",
      "4854012d419d4b2ca0b9b23bc9b8c0e9",
      "bcd9bdc606784039af1ed36b9c7fdd4e",
      "bf313cf03ad5458792e61b4fff62b3b0"
     ]
    },
    "id": "u515SJs2uDwS",
    "outputId": "fc351245-7f17-463f-e2eb-23b4e2bc425f"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_train_emb = model.encode(train_texts, show_progress_bar=True)\n",
    "X_test_emb = model.encode(test_texts, show_progress_bar=True)\n",
    "\n",
    "clf_emb = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
    "clf_emb.fit(X_train_emb)\n",
    "\n",
    "y_pred_emb_raw = clf_emb.predict(X_test_emb)\n",
    "y_pred_emb = np.where(y_pred_emb_raw == -1, 1, 0)\n",
    "\n",
    "precision_emb = precision_score(y_test, y_pred_emb)\n",
    "recall_emb = recall_score(y_test, y_pred_emb)\n",
    "\n",
    "print(f\"Embeddings Precision: {precision_emb:.4f}\")\n",
    "print(f\"Embeddings Recall: {recall_emb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukGE7udQuDwS"
   },
   "source": [
    "Подготовьте выборку: удалите столбцы `['id', 'date', 'price', 'zipcode']`, сформируйте обучающую и тестовую выборки по 10 тысяч домов.\n",
    "\n",
    "Добавьте в тестовую выборку 10 новых объектов, в каждом из которых испорчен ровно один признак — например, это может быть дом из другого полушария, из далёкого прошлого или будущего, с площадью в целый штат или с таким числом этажей, что самолётам неплохо бы его облетать стороной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-0yaOlyuDwS"
   },
   "source": [
    "Посмотрим на методы обнаружения аномалий на более простых данных — уж на табличном датасете с 19 признаками всё должно работать как надо!\n",
    "\n",
    "Скачайте данные о стоимости домов: https://www.kaggle.com/harlfoxem/housesalesprediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lg-T_gYEuDwS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "df_clean = df.drop(columns=['id', 'date', 'price', 'zipcode'])\n",
    "clean_data = df_clean.sample(n=20000, random_state=42)\n",
    "train_df = clean_data.iloc[:10000]\n",
    "test_df_clean = clean_data.iloc[10000:]\n",
    "\n",
    "anomalies_list = []\n",
    "base_house = test_df_clean.iloc[0].copy()\n",
    "\n",
    "a1 = base_house.copy(); a1['lat'] = -33.0; anomalies_list.append(a1)\n",
    "a2 = base_house.copy(); a2['yr_built'] = 1200; anomalies_list.append(a2)\n",
    "a3 = base_house.copy(); a3['yr_built'] = 2050; anomalies_list.append(a3)\n",
    "a4 = base_house.copy(); a4['sqft_living'] = 100000; anomalies_list.append(a4)\n",
    "a5 = base_house.copy(); a5['floors'] = 100; anomalies_list.append(a5)\n",
    "a6 = base_house.copy(); a6['bathrooms'] = 200; anomalies_list.append(a6)\n",
    "a7 = base_house.copy(); a7['sqft_living'] = 0; a7['sqft_above'] = 0; anomalies_list.append(a7)\n",
    "a8 = base_house.copy(); a8['sqft_living15'] = 50000; a8['sqft_lot15'] = 100; anomalies_list.append(a8)\n",
    "a9 = base_house.copy(); a9['grade'] = 100; anomalies_list.append(a9)\n",
    "a10 = base_house.copy(); a10['long'] = 0.0; anomalies_list.append(a10)\n",
    "\n",
    "test_anomalies_df = pd.DataFrame(anomalies_list)\n",
    "test_df_final = pd.concat([test_df_clean, test_anomalies_df], ignore_index=True)\n",
    "\n",
    "# Метки\n",
    "y_test_house = np.array([0]*len(test_df_clean) + [1]*len(test_anomalies_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkkxswUOuDwS"
   },
   "source": [
    "**Задание 9. (2 балла)**\n",
    "\n",
    "Примените IsolationForest для поиска аномалий в этих данных, запишите их качество (как и раньше, это precision и recall). Проведите исследование:\n",
    "\n",
    "Нарисуйте распределения всех признаков и обозначьте на этих распределениях объекты, которые признаны аномальными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-q8DQDG7uDwS",
    "outputId": "9ada0fb0-16cd-41b1-ffd6-ff5e65a801f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_train_house = train_df\n",
    "X_test_house = test_df_final\n",
    "feature_predictions = np.zeros((len(X_train_house.columns), len(X_test_house)))\n",
    "for t in range(1, 10): #times to ensemble\n",
    "\n",
    "  for i, column in enumerate(X_train_house.columns):\n",
    "\n",
    "    X_train_single = X_train_house[[column]]\n",
    "    X_test_single = X_test_house[[column]]\n",
    "\n",
    "    clf = IsolationForest(\n",
    "        n_estimators=1000,\n",
    "        contamination=0.00001,\n",
    "        random_state=666\n",
    "    )\n",
    "    clf.fit(X_train_single)\n",
    "\n",
    "    pred = clf.predict(X_test_single)\n",
    "    feature_predictions[i, :] = np.where(pred == -1, 1, 0)\n",
    "\n",
    "y_pred_ensemble = np.max(feature_predictions, axis=0)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test_house, y_pred_ensemble)\n",
    "recall = recall_score(y_test_house, y_pred_ensemble)\n",
    "\n",
    "print(f\"Ensemble Precision: {precision:.4f}\")\n",
    "print(f\"Ensemble Recall:    {recall:.4f}\")\n",
    "\n",
    "found_true = np.sum((y_pred_ensemble == 1) & (y_test_house == 1))\n",
    "found_false = np.sum((y_pred_ensemble == 1) & (y_test_house == 0))\n",
    "\n",
    "print(f\"Найдено истинных аномалий: {found_true} из 10\")\n",
    "print(f\"Ложных срабатываний:      {found_false}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
